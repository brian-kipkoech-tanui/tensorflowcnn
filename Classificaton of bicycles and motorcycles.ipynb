{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2cb2b5",
   "metadata": {},
   "source": [
    "# Computer Vision Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429c1f3",
   "metadata": {},
   "source": [
    "- Studies of the visual cortex inspired the neocognitron, which gradually evolved into Convolutional Neural Networks.\n",
    "- Why not use deep neural network with fully connected layers for image recognition tasks?\n",
    "- * Breaks down for larger images because of the huge number of parameters it requires.\n",
    "- * CNNs solves this problem using partially connected layers and weight sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25a979",
   "metadata": {},
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd5da5",
   "metadata": {},
   "source": [
    "- Neurons in the first Convolutional layer are not connected to every single pixel in the input image, but only to pixels in their receprive field.\n",
    "- Each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer.\n",
    "- This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger high-level features in the nect hidden ayer, and so on.\n",
    "- In order for a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs (Zero Padding)\n",
    "- The shift from one receprive field to the next is called a stride.\n",
    "- Example:\n",
    "> A 5 by 5 input layer could be connected to a 3 by 4 layer, using a 3 by 3 receptive fields and  a stride of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911e99d",
   "metadata": {},
   "source": [
    "## Filters/Convolution Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fb111",
   "metadata": {},
   "source": [
    "- A layer full of neurons using the same filter outputs a feature map,which highlights the areas in an image that activate the filter the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8c5f1",
   "metadata": {},
   "source": [
    "## Stacking Multiple Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb171b55",
   "metadata": {},
   "source": [
    "- A convolutional layer has multiple filters and outputs one feature map per filter.\n",
    "- A convolutional layer simultenously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs.\n",
    "- The fact that all neurons in a feature map share the same parameters dramatically reduces the number of parameters in the model.\n",
    "\n",
    "- Once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859843c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b861f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990139d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_cifar_data(url, filename=\"cifar.tar.gz\"):\n",
    "    \"\"\"A function for extracting the CIFAR-100 dataset and storing it as a gzipped file\n",
    "    \n",
    "    Arguments:\n",
    "    url      -- the URL where the dataset is hosted\n",
    "    filename -- the full path where the dataset will be written\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Todo: request the data from the data url\n",
    "    # Hint: use `requests.get` method\n",
    "    r = requests.get(url)\n",
    "    with open(filename, \"wb\") as file_context:\n",
    "        file_context.write(r.content)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_cifar_data(\"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# with tarfile.open(\"cifar.tar.gz\", \"r:gz\") as tar:\n",
    "#     tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"./cifar-100-python/meta\", \"rb\") as f:\n",
    "#     dataset_meta = pickle.load(f, encoding='bytes')\n",
    "\n",
    "# with open(\"./cifar-100-python/test\", \"rb\") as f:\n",
    "#     dataset_test = pickle.load(f, encoding='bytes')\n",
    "\n",
    "# with open(\"./cifar-100-python/train\", \"rb\") as f:\n",
    "#     dataset_train = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b872009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feel free to explore the datasets\n",
    "\n",
    "# dataset_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a077b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Each 1024 in a row is a channel (red, green, then blue)\n",
    "# row = dataset_train[b'data'][6]\n",
    "# red, green, blue = row[0:1024], row[1024:2048], row[2048:]\n",
    "\n",
    "# # Each 32 items in the channel are a row in the 32x32 image\n",
    "# red = red.reshape(32,32)\n",
    "# green = green.reshape(32,32)\n",
    "# blue = blue.reshape(32,32)\n",
    "\n",
    "# # Combine the channels into a 32x32x3 image!\n",
    "# combined = np.dstack((red,green,blue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  All in one:\n",
    "# test_image = np.dstack((\n",
    "#     row[0:1024].reshape(32,32),\n",
    "#     row[1024:2048].reshape(32,32),\n",
    "#     row[2048:].reshape(32,32)\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_image);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Todo: Filter the dataset_train and dataset_meta objects to find the label numbers for Bicycle and Motorcycles\n",
    "desired_label_nos = {x:idx for idx, x in enumerate(dataset_meta[b'fine_label_names']) if x==b'bicycle' or x==b'motorcycle'}\n",
    "# Label numbers for bicycle and motorcycles\n",
    "desired_label_nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ef525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({\n",
    "    \"filenames\": dataset_train[b'filenames'],\n",
    "    \"labels\": dataset_train[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_train[b'filenames']))\n",
    "})\n",
    "\n",
    "# Drop all rows from df_train where label is not 8 or 48\n",
    "df_train_bike = df_train.loc[(df_train.labels==8),:]   #TODO: Fill in\n",
    "df_train_motorbike = df_train.loc[(df_train.labels==48),:]\n",
    "\n",
    "# Decode df_train.filenames so they are regular strings\n",
    "df_train_bike[\"filenames\"] = df_train_bike[\"filenames\"].apply(\n",
    "    lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "df_train_motorbike[\"filenames\"] = df_train_motorbike[\"filenames\"].apply(\n",
    "    lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"filenames\": dataset_test[b'filenames'],\n",
    "    \"labels\": dataset_test[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_test[b'filenames']))\n",
    "})\n",
    "\n",
    "# Drop all rows from df_test where label is not 8 or 48\n",
    "df_test_bike = df_test.loc[(df_test.labels==8),:]   #TODO: Fill in\n",
    "df_test_motorbike = df_test.loc[(df_test.labels==48),:]\n",
    "\n",
    "# Decode df_test.filenames so they are regular strings\n",
    "df_test_bike[\"filenames\"] = df_test_bike[\"filenames\"].apply(\n",
    "    lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "df_test_motorbike[\"filenames\"] = df_test_motorbike[\"filenames\"].apply(\n",
    "    lambda x: x.decode(\"utf-8\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_images(imgs_details_df, imgs_dataset, target_folderpath):\n",
    "    '''\n",
    "    This function takes the following inputs and saves all images to the 'target_folder' in the local directory\n",
    "    inputs:\n",
    "    imgs_details_df: DataFrame containing details of images like filenames, finelabels (specific image labels) \n",
    "                     and row number details \n",
    "    imgs_dataset   : Original CIFAR-100 images dataset containing image data in the row major form that further \n",
    "                     needs to be transformed into 3 channeled image array\n",
    "    target_folder  : Output folder where the transformed images are saved\n",
    "    '''\n",
    "    \n",
    "    # Saving dataset 'row' numbers of corresponding images from 'imgs_details_df' dataframe\n",
    "    row_number_imgs_data = imgs_details_df['row'] \n",
    "    \n",
    "    # Corresponding image filenames in the dataset\n",
    "    imgs_file_names = imgs_details_df['filenames']\n",
    "    \n",
    "    for row, img_name in zip(row_number_imgs_data, imgs_file_names):\n",
    "        \n",
    "        #Grab the image data in row-major form\n",
    "        img = imgs_dataset[b'data'][row]\n",
    "\n",
    "        # Consolidated stacking/reshaping from earlier (For all 3-R,G,B channels)\n",
    "        target = np.dstack((\n",
    "                        img[0:1024].reshape(32,32,1),\n",
    "                        img[1024:2048].reshape(32,32,1),\n",
    "                        img[2048:].reshape(32,32,1)\n",
    "                         ))\n",
    "    \n",
    "        # Save the image to target folder (local directory)\n",
    "        try:\n",
    "            save_img_filepath = os.path.join(target_folderpath, img_name)\n",
    "            plt.imsave(save_img_filepath, target)\n",
    "            \n",
    "            # For printing the saved image filepath in correct format\n",
    "            raw_save_img_filepath = r\"{}\".format(save_img_filepath)\n",
    "            nraw_save_img_filepath = os.path.normpath(raw_save_img_filepath)\n",
    "            print(f\"Saved: {img_name} to {nraw_save_img_filepath}\")\n",
    "            \n",
    "        # Return any signal data you want for debugging\n",
    "        except RuntimeError as e:\n",
    "            return f\"Error: {e} \\nAn error encountered while saving {img_name} file to the target folder located at {target_folderpath}\"\n",
    "\n",
    "## TODO: save ALL images using the save_images function\n",
    "\n",
    "# Save all transformed train dataset images in \"./train\"\n",
    "save_images(imgs_details_df= df_train_bike,\n",
    "            imgs_dataset= dataset_train, \n",
    "            target_folderpath= \"./dataset/train/bicycle\")\n",
    "save_images(imgs_details_df= df_train_motorbike,\n",
    "            imgs_dataset= dataset_train, \n",
    "            target_folderpath= \"./dataset/train/motorcycle\")\n",
    "\n",
    "# Save all transformed test dataset images in \"./test\"\n",
    "save_images(imgs_details_df= df_test_bike,\n",
    "            imgs_dataset= dataset_test, \n",
    "            target_folderpath= \"./dataset/test/bicycle\")\n",
    "save_images(imgs_details_df= df_test_motorbike,\n",
    "            imgs_dataset= dataset_test, \n",
    "            target_folderpath= \"./dataset/test/motorcycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f507ab3",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db49f80",
   "metadata": {},
   "source": [
    "## Preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690cdebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    './dataset/train/',\n",
    "    target_size=(32, 32),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913a46f",
   "metadata": {},
   "source": [
    "## Preprocessing the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a84cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    './dataset/test',\n",
    "    target_size=(32,32),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e5a63",
   "metadata": {},
   "source": [
    "# Building the cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42750f49",
   "metadata": {},
   "source": [
    "## Initializing the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d15a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186d774",
   "metadata": {},
   "source": [
    "## Step1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193074a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu',\n",
    "    input_shape=[32,32,3]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5616c93",
   "metadata": {},
   "source": [
    "## Step2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16374ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2,\n",
    "    strides=2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03442845",
   "metadata": {},
   "source": [
    "## Adding a second Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef8dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu'\n",
    "))\n",
    "# add a maxpooling layer\n",
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2,\n",
    "    strides=2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f896e",
   "metadata": {},
   "source": [
    "## Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e15769a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e98ed6",
   "metadata": {},
   "source": [
    "## Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3a05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(\n",
    "    units=128,\n",
    "    activation='relu'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086413e9",
   "metadata": {},
   "source": [
    "## Step 5 - Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ef4520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(\n",
    "    units=1,\n",
    "    activation='sigmoid'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f438ab0",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ac0c5",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7ba80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104f684",
   "metadata": {},
   "source": [
    "## Training the cnn on the training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d099b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 5s 126ms/step - loss: 0.5860 - accuracy: 0.6920 - val_loss: 0.4746 - val_accuracy: 0.7300\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 2s 63ms/step - loss: 0.4558 - accuracy: 0.7890 - val_loss: 0.3888 - val_accuracy: 0.8100\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.4423 - accuracy: 0.8110 - val_loss: 0.3374 - val_accuracy: 0.8650\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 2s 66ms/step - loss: 0.4104 - accuracy: 0.8020 - val_loss: 0.4320 - val_accuracy: 0.8000\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.3466 - accuracy: 0.8440 - val_loss: 0.3710 - val_accuracy: 0.8500\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.3637 - accuracy: 0.8320 - val_loss: 0.3130 - val_accuracy: 0.8600\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.3560 - accuracy: 0.8540 - val_loss: 0.2954 - val_accuracy: 0.8900\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.3562 - accuracy: 0.8450 - val_loss: 0.3263 - val_accuracy: 0.8650\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 2s 64ms/step - loss: 0.3222 - accuracy: 0.8530 - val_loss: 0.4871 - val_accuracy: 0.7750\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.3126 - accuracy: 0.8720 - val_loss: 0.4397 - val_accuracy: 0.8150\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 2s 66ms/step - loss: 0.3123 - accuracy: 0.8700 - val_loss: 0.4090 - val_accuracy: 0.8300\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 2s 64ms/step - loss: 0.3134 - accuracy: 0.8580 - val_loss: 0.2854 - val_accuracy: 0.8750\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 2s 66ms/step - loss: 0.2942 - accuracy: 0.8750 - val_loss: 0.2861 - val_accuracy: 0.8750\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 2s 63ms/step - loss: 0.3150 - accuracy: 0.8630 - val_loss: 0.4066 - val_accuracy: 0.8450\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 2s 66ms/step - loss: 0.3163 - accuracy: 0.8700 - val_loss: 0.2786 - val_accuracy: 0.8800\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 2s 64ms/step - loss: 0.2732 - accuracy: 0.8830 - val_loss: 0.2746 - val_accuracy: 0.8850\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 2s 69ms/step - loss: 0.2728 - accuracy: 0.8820 - val_loss: 0.3645 - val_accuracy: 0.8650\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.2585 - accuracy: 0.8800 - val_loss: 0.2603 - val_accuracy: 0.9000\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 0.2583 - accuracy: 0.8950 - val_loss: 0.2707 - val_accuracy: 0.8950\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.2779 - accuracy: 0.8710 - val_loss: 0.2798 - val_accuracy: 0.8850\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 2s 66ms/step - loss: 0.2425 - accuracy: 0.9000 - val_loss: 0.3311 - val_accuracy: 0.8650\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 2s 63ms/step - loss: 0.2365 - accuracy: 0.9000 - val_loss: 0.3341 - val_accuracy: 0.8750\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.2445 - accuracy: 0.8980 - val_loss: 0.3092 - val_accuracy: 0.8650\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.2361 - accuracy: 0.8960 - val_loss: 0.3299 - val_accuracy: 0.8900\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.2387 - accuracy: 0.9040 - val_loss: 0.4861 - val_accuracy: 0.8150\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.2284 - accuracy: 0.9020 - val_loss: 0.2844 - val_accuracy: 0.8900\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.2136 - accuracy: 0.9040 - val_loss: 0.2953 - val_accuracy: 0.8750\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.2284 - accuracy: 0.8970 - val_loss: 0.2968 - val_accuracy: 0.8950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x224f5737308>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data=test_set, epochs=100, callbacks=tf.keras.callbacks.EarlyStopping(patience=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266bcee",
   "metadata": {},
   "source": [
    "# Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b565122b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step\n",
      "{'bicycle': 0, 'motorcycle': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img(\n",
    "    './dataset/sample/bicycle_or_motorcycle_0000126.png',\n",
    "    target_size=(32,32)\n",
    ")\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "print(training_set.class_indices)\n",
    "\n",
    "if result[0][0] == 0:\n",
    "    prediction='bicycle'\n",
    "else:\n",
    "    prediction='motorcycle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "310078e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01ec8b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motorcycle\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e50d1f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "{'bicycle': 0, 'motorcycle': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img(\n",
    "    './dataset/sample/bicycle_or_motorcycle_000791.png',\n",
    "    target_size=(32,32)\n",
    ")\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "print(training_set.class_indices)\n",
    "\n",
    "if result[0][0] == 0:\n",
    "    prediction='bicycle'\n",
    "else:\n",
    "    prediction='motorcycle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a75c4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bicycle\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d10859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
